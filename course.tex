\documentclass{beamer}
\usepackage{graphicx}
\usepackage{multicol}
\usepackage{amsmath}
\usepackage{booktabs}

\graphicspath{{latex/}{img/}}

% for custom colors
\definecolor{maroon}{RGB}{215,0,0}

\mode<presentation>
{
    % theme + colorscheme
    \usetheme{Madrid}
    \usecolortheme{beaver}

    % customize bullets in items list
    \setbeamertemplate{itemize item}{\color{red}$\bullet$}
    \setbeamertemplate{itemize subitem}{\color{red}$\circ$}

    % customize bullets in table of content
    \setbeamerfont{section number projected}{%
      family=\sffamily,series=\bfseries,size=\normalsize}
    \setbeamercolor{section number projected}{bg=maroon, fg=white}
    \setbeamercolor{subsection number projected}{bg=maroon}
    \setbeamertemplate{section in toc}[circle]
    \setbeamertemplate{subsection in toc}[square]

    % remove navigation symbols
    \setbeamertemplate{navigation symbols}{}
}

% customize footline: print section/subsection/date+slides_number instead of
% author/short_title/date+slides_number.
% You might need to remove it if you use another theme than Madrid.
\makeatletter
\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.33333\paperwidth,ht=2.25ex,dp=1ex,center]
    {section in head/foot}\usebeamerfont{section in head/foot}\insertsection
  \end{beamercolorbox}% if no % at the end, there is a gap between subboxes
  \begin{beamercolorbox}[wd=.33333\paperwidth,ht=2.25ex,dp=1ex,center]
    {title in head/foot}\usebeamerfont{title in head/foot}\insertsubsection
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.33333\paperwidth,ht=2.25ex,dp=1ex,right]
    {date in head/foot}\usebeamerfont{date in head/foot}\insertshortdate{}
    \hspace*{2em} \insertframenumber{} / \inserttotalframenumber\hspace*{2ex}
  \end{beamercolorbox}}%
  \vskip0pt
}
\makeatother

% main information
\title[Algorithms for Data Analysis]{Algorithms for Data Analysis}
\author{Julien Tissier}
\date{\today}

\begin{document}

%%%%%%%%%%%%%%%%%%%%%%%
%%    F R A M E S    %%
%%%%%%%%%%%%%%%%%%%%%%%


% section -> on the left in footline; subsection -> in the center footline
\section*{\insertshortauthor} % asterisk -> do not appear in table of content
\subsection*{\insertshorttitle}

% frame with title + main information
\begin{frame}
    \titlepage
\end{frame}

% summary frame
\begin{frame}
  \frametitle{Overview}
  \begin{multicols}{2}
    \tableofcontents
  \end{multicols}
\end{frame}

%------------------
%    I N T R O    -
%------------------

\section{Introduction to Data Analysis}

% data (analysis) definition
\subsection{Definition}
\begin{frame}
  \frametitle{What is data analysis?}
  \begin{block}{Data}
    Pieces of information (measurement, values, facts...) that can be :
    \begin{itemize}
      \item structured (matrices, tabular data, RDBMS, time series...)
      \item unstructured (news articles, webpages, images/video...)
    \end{itemize}
  \end{block}
  \begin{block}{Data Analysis}
  Process of preparing, transforming and using models to find more information
  from data, as well as visualizing results.
  \end{block}
\end{frame}

% tools (why Python?)
\subsection{Tools and libraries}
\begin{frame}
  \frametitle{How to analyze data?}
  There are usually two steps in data analysis. The first one is to find and
  \textbf{develop models} that can extract useful information from data (with
  languages like R or MATLAB). The second one is to \textbf{develop programs}
  that can be used in production systems (with languages like Java or C++).
  \\~\\

  With a growing popularity among scientists as well as the development of
  efficient libraries (numpy, pandas), \textbf{Python} became a great tool for
  data analysis. Python has many advantages :
  \begin{itemize}
    \item great for string/data processing
    \item can be used for both prototyping/production
    \item has a lot of existing libraries
    \item can easily integrate C/C++/FORTRAN legacy code
    \item easy to read/develop
  \end{itemize}
\end{frame}

% list of libraries
\begin{frame}
  \frametitle{Libraries}
  This course will be based on Python 3.5 (or above) and the following
  libraries :
  \begin{itemize}
    \item IPython (7.0+) : enhanced Python shell
    \item numpy (1.15+) : fast/efficient arrays and operations
    \item pandas (0.23+) : data structures (Series/DataFrame)
    \item matplotlib (3.0+) : plots and 2D visualization
    \item scipy (1.1.0+) : scientific algorithms
    \item scikit-learn (0.20+) : machine learning algorithms
  \end{itemize}

  Jupyter Notebook will also be used to give you samples of code, as they
  provide a more interactive way to learn and discover how these libraries
  work.
\end{frame}

% numpy
\subsection{Numpy}
\begin{frame}
  \frametitle{Numpy}
  Numpy (\textbf{Num}erical \textbf{Py}thon) is a high performance scientific
  computing library that can be used for matrices computations, Fourier
  transforms, linear algebra, statistical computations...
  \\~\\

  The main type of data in Numpy is the \textbf{ndarray} :
  \begin{itemize}
    \item n-dimensional array
    \item fixed size
    \item homogeneous datatypes
    \item similar to C arrays (continuous block of memory)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Why is Numpy efficient?}
  As a high level language, Python is slow to do any heavy computations,
  especially if very large arrays are involved. Numpy solves this problem thanks
  to the ndarray datatypes :
  \begin{itemize}
    \item efficient memory management (continuous block)
    \item use C loops instead of Python loops for computations on array
    \item vectorized operations (computations are done block by block, not
      element by element)
    \item rely on low-level routines for some operations (BLAS/LAPACK)
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Numpy}
  \begin{example}
    \begin{verbatim}

  import numpy as np

  a = np.array([1, 2, 3, 4])
  b = np.array([6, 7, 8, 9])

  c = a * b

  d = np.array([[1, 2], [3, 4]])
    \end{verbatim}
  \end{example}
\end{frame}

% pandas
\subsection{Pandas}
\begin{frame}
  \frametitle{Pandas}
  Pandas is a high-performance Python library used to work with data and analyze
  them. It contains many pre-implemented methods to read and parse data, as well
  as common statistical computations (mean, variance, correlation...).
  \\~\\

  Pandas has two main datatypes:
  \begin{itemize}
    \item Series : one-dimensional container. Indexes can be integers (like an
      array) or other objects (string, date...)
    \item DataFrame : tabular data, like a spreadsheet. It contains multiple
      rows and multiple columns. It can be seen as a collection of Series.
  \end{itemize}
\end{frame}

\begin{frame}[fragile]
  \frametitle{Pandas}
  \begin{example}[Series]
    \begin{verbatim}

  from pandas import Series

  s1 = Series([4, 7, 9, -1])
  s2 = Series([12, 3.2, "John"],
              index=["mark", "gpa", "name"])
    \end{verbatim}
  \end{example}

  \begin{example}[DataFrame]
    \begin{verbatim}

  from pandas import DataFrame
  df = DataFrame({"key1": [1, 2, 3],
                  "key2": [4, 5, 6]})
    \end{verbatim}
  \end{example}
\end{frame}

% matplotlib
\subsection{Matplotlib}
\begin{frame}
  \frametitle{Matplotlib}
  Matplotlib is a plotting library used to visualize data and create graphics.
  Pandas directly uses matplotlib for representation.
  \begin{figure}[h]
    \includegraphics[width=8cm]{img/matplotlib_1.png}
  \end{figure}
\end{frame}

\begin{frame}
  \frametitle{Matplotlib}
  Matplotlib is a plotting library used to visualize data and create graphics.
  Pandas directly uses matplotlib for representation.
  \begin{figure}[h]
    \includegraphics[width=8cm]{img/matplotlib_2.png}
  \end{figure}
\end{frame}


%----------------------------------------
%    M A C H I N E   L E A R N I N G    -
%----------------------------------------

\section{Machine Learning}

% ML definition
\subsection{Definition}
\begin{frame}
  \frametitle{What is Machine Learning?}
  \begin{block}{Machine Learning (ML)}
    ML is when a computational system can \textbf{automatically learn} and
    extract \textbf{knowledge from data} without being explicitly programmed.
    ML is at the intersection of statistics, artificial intelligence and
    computer science.

    ML is now everywhere:
    \begin{itemize}
      \item automatic friend tagging in your Facebook photos
      \item music recommendation for Spotify/YouTube
      \item self-driven cars
      \item medical diagnosis in a hospital
      \item and many more...
    \end{itemize}
    ML can be \textbf{supervised} or \textbf{unsupervised}.
  \end{block}
\end{frame}

% ML history
\begin{frame}
  \frametitle{What is Machine Learning?}
  \begin{alertblock}{Brief history}
    At the beginning, many decision-making applications (like spam detection)
    were built with a lot of manually programmed if/else conditions. These
    methods have some drawbacks :
    \begin{itemize}
      \item it requires the \textbf{knowledge of an expert} to design the rules
      \item it is \textbf{very specific} to a task
    \end{itemize}

    With ML, you only need to feed a model with your data. The algorithm will
    find the rules by itself.
  \end{alertblock}
\end{frame}

% Supervised
\subsection{Supervised vs. Unsupervised}
\begin{frame}
  \frametitle{Supervised Learning}
  In supervised  learning, you give the algorithm two kinds of data :
  \begin{itemize}
    \item \textbf{inputs} (an image, information about a person...)
    \item \textbf{outputs} (name of the object, the salary...)
  \end{itemize}

  The algorithm learns to find a way to \textbf{produce the output given the
  input}. The algorithm is then capable of producing the output of an input it
  has never seen before. \\~\\
  Supervised learning can be used to :
  \begin{itemize}
    \item recognize handwritten digits (input=image, output=digit)
    \item identify spams (input=mail, output=yes/no)
    \item predict a price (input=house infomation, output=price)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Supervised Learning}
  The inputs are in general real value vectors (sometimes it can be binary
  vectors). Each dimension of this vector is
  called a \textbf{feature}. \\~\\

  There are two main problems that can be solved with supervised learning :
  \begin{itemize}
    \item \textbf{classification} : the output is the class the item belongs to.
      It can be a binary classification problem (yes/no) or a multi-class
      classification problem (class 1, class 2, class 3...)
    \item \textbf{regression} : the output is a real value number
  \end{itemize}
\end{frame}

% Unsupervised
\begin{frame}
  \frametitle{Unsupervised Learning}
  In unsupervised  learning, there are no known output (or label), so you only give the algorithm
  the inputs. For example, if you want to regroup similar clients according to
  their customer habits, you do not know in advance what are the groups.\\~\\

  The main problems that can be solved with unsupervised learning are :
  \begin{itemize}
    \item \textbf{clustering}
    \item \textbf{dimensionality reduction}
  \end{itemize}

  The main difficulty of unsupervised learning algorithms is to \textbf{evaluate
  them}. Since we do not have any labels, there are not any "true" valid answer.
  Most of the time, we need to manually evaluate if the output of the algoritm
  is relevant. Sometimes, unsupervised learning algorithms are used as a
  \textbf{pre-processing} step before supervised learning algorithms and can
  lead lead to a more accurate model or a faster training.
\end{frame}

% Evaluation
\subsection{Training, test \& validation sets}
\begin{frame}
  \frametitle{How to evaluate a ML algorithm?}
  After training a ML algorithm with data, we need to evaluate its performance.
  One way would be to feed the algorithm with the same data, compute the output
  and compare it with the original output.\\~\\

  This method is bad because it does not tell us the capacity of the algorithm
  \textbf{to generalize} (is the algorithm able to perform well on unseen data
  ?). One way to solve this problem is to separate the data into 3 sets :
  \begin{itemize}
    \item training set : used to train the algorithm
    \item validation set : used to adjust the algorithm
    \item test set : used to measure the performance of the algorithm
  \end{itemize}
\end{frame}

% overfitting/underfitting
\subsection{Overfitting/Underfitting}
\begin{frame}
  \frametitle{Problems of ML}
  \begin{itemize}
    \item \textbf{underfitting} : when the model is too simple. The training
    score and the test score are both bad (left)
    \item \textbf{overfitting} : when the model is too complex (right). The
    training score is good but the test score is bad. The model is not able to
    generalize well.
  \end{itemize}
  \begin{center}
    \includegraphics[height=4.7cm]{img/underfitting.png}
  \end{center}
\end{frame}


%----------------------------
%    S U P E R V I S E D    -
%----------------------------

\section{Supervised Learning}

% KNN
\subsection{k-Nearest Neighbors}
\begin{frame}
  \frametitle{k-Nearest Neighbors - Classification}
  \textbf{Inputs} : set of training points $\mathcal{S}$, point $X$ to
  classify\\
  \textbf{Procedure} :
  \begin{itemize}
    \item find the $k$ closest points (the neighbors) to $X$ from $\mathcal{S}$
    \item find the most frequent class $c$ among the neighbors
    \item assign $c$ to $X$
  \end{itemize}

  \begin{columns}
    \column{0.48\linewidth}
      \parbox{\linewidth}{The $k$ closest points are defined as the points with
      minimal distance (usually the Euclidean distance) to $X$. If there is a
      tie in finding $c$, we can increase/decrease $k$ until the tie is broken.}

    \column{0.48\linewidth}
      \centering
      \includegraphics[height=3.8cm]{img/knn.png}
    \end{columns}
\end{frame}

\begin{frame}
  \frametitle{k-Nearest Neighbors - Regression}
  \textbf{Inputs} : set of training points $\mathcal{S}$, point $X$ to predict
  value

  \textbf{Procedure} :
  \begin{itemize}
    \item find the $k$ closest points (the neighbors) to $X$ from $\mathcal{S}$
    \item compute the average value $v$ of all neighbors
    \item assign $v$ to $X$
  \end{itemize}

  If $k$ is set to 1, the value assigned to $X$ is simply the value of its
  nearest neighbor.
\end{frame}

\begin{frame}
  \frametitle{k-Nearest Neighbors}
  The k-nearest neighbors is one of the simplest model in ML. It usually has two
  main parameters : \textbf{the number of neighbors} used and \textbf{the
  distance} used to find the closest points.
  \\~\\
  \textbf{Pros}
  \begin{itemize}
    \item model is easy to understand
    \item does not require extended tuning to achieve good performance
    \item works well as a baseline before training more complex models
  \end{itemize}

  \textbf{Cons}
  \begin{itemize}
    \item slow when many training points or many features
    \item not very good on sparse data
  \end{itemize}
\end{frame}

% Linear models
\subsection{Linear Models}
\begin{frame}
  \frametitle{Linear Models - Regression}
  Linear models are usually used to make values prediction. The output $\hat{y}$
  is a \textbf{linear function} of each features $x_i$ of a given input vector
  $X$ (hence the name of \textbf{linear} models).

  \begin{equation*}
    \hat{y} = \sum_{i = 1}^k w_i * x_i + b
  \end{equation*}

  \begin{columns}
    \column{0.48\linewidth}
      \parbox{\linewidth}{The model learns the coefficients $w_i$ and $b$. The
      model is trained to minimize the \textbf{Mean Squared Error (MSE)} on all
      training points $X$ in $\mathcal{S}$ (where $y$ is the correct value of
      $X$).

      \begin{equation*}
        MSE = \frac{1}{\#\mathcal{S}} \sum_{X \in \mathcal{S}} (\hat{y} - y)^2
      \end{equation*}
      }

    \column{0.48\linewidth}
      \centering
      \includegraphics[height=3.6cm]{img/linear_reg.png}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Linear Models - Regression}
  \begin{itemize}
    \item \underline{Training}\\
      \textbf{Inputs} : set of training points $\mathcal{S}$\\
      \textbf{Procedure} :
      \begin{itemize}
        \item compute the MSE
        \item compute the partial derivatives $\frac{\partial MSE}{\partial
          w_i}$ and  $\frac{\partial MSE}{\partial b}$
        \item solve the linear system given by the equations $\frac{\partial
          MSE}{\partial w_i}=0$ and $\frac{\partial MSE}{\partial b}=0$
        \item the solution is unique and gives you the $w_i$ and $b$

      \end{itemize}
      \textbf{Output} : learned $w_i$ and $b$

    \item \underline{Predicting}\\
      \textbf{Inputs} : learned $w_i$ and $b$, point $X$ to predict value\\
      \textbf{Procedure} : compute $\hat{y}$ with the formula and $w_i$, $b$,
      $x_i$\\
      \textbf{Output} : predicted value $\hat{y}$

  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Linear Models - Classification}
  Linear models can also be used for either binary or multi-class classification
  problems. For binary classification (1 or 0 labels), the model evaluates the
  output with :

  \begin{equation*}
    \hat{y} =
      \begin{cases}
        1 & \text{if } w \cdot x + b \geq 0\\
        0 & \text{otherwise}
      \end{cases}
  \end{equation*}

  We can rewrite the condition to remove the bias $b$ with $\sum_{i = 0}^k w_i *
  x_i \geq 0$ and set $x_0$ to 1 for all vectors $X$. The objective of the model
  is to find a good $w$ to predict the correct label (such that $\hat{y} =
  y$). Different methods exist to find $w$ :
  \begin{itemize}
    \item perceptron
    \item logistic regression
    \item support vector machines (SVM)
  \end{itemize}
\end{frame}

\begin{frame}
  \frametitle{Linear Models - Perceptron}
  We can define a loss $\ell$ (called the zero-one loss) for each point $(x, y)$
  from our training set as follows :

  \begin{equation*}
    \ell(x, y, w) =  1_{[(w \cdot x)y \leq 0]}
  \end{equation*}

  The perceptron algorithm learns the $w$ weights as follows :
  \begin{itemize}
    \item initialize $w$ to 0
    \item \textbf{for} step in [1..n]
      \begin{itemize}
        \item \textbf{for} x in training dataset
        \begin{itemize}
          \item[] \textbf{if $ (w \cdot x) \times y \leq 0$}\\
            \hspace{.5cm} w = w + $\lambda$ y $\cdot$ x
        \end{itemize}
      \end{itemize}
  \end{itemize}

  The perceptron algorithm only updates the weights $w$ when it encounters a
  misclassified example. If the dataset is \textbf{linearly separable}, the
  perceptron algorithm is guaranteed to find an optimal solution.
\end{frame}

\begin{frame}
  \frametitle{Linear Models - Logistic Regression}
  Instead of directly finding the output $\hat{y}$ with the sign function, we
  can compute the \textbf{probability of belonging to class 1} defined as
  follows :

  \begin{equation*}
    P(y=1 | x) = \sigma(w \cdot x) = \frac{1}{1 + e^{-w \cdot x}} \in [0, 1]
  \end{equation*}

  The idea is to find $w$ such that this probability is high (near 1) for points
  labeled with 1, and low (near 0) for points labeled with 0. We can define
  another loss function (called negative log-likelihood) :

  \begin{equation*}
    \ell(x, y, w) = - y \times \log(\sigma(w \cdot x))
                 - (1 - y) \times \log(1 - \sigma(w \cdot x))
  \end{equation*}

  The model is trained to minimize the loss for all training examples. Since
  this loss function is convex, we can use different algorithms like gradient
  descent, L-BFGS...
\end{frame}

\begin{frame}
  \frametitle{Linear Models}
  Linear models have two
  main parameters : \textbf{the weight of regularization} and \textbf{the
  type of regularization} used.
  \\~\\
  \textbf{Pros}
  \begin{itemize}
    \item models are fast to train and predict
    \item can be used with large datasets (scalability)
    \item works well on sparse data
  \end{itemize}

  \textbf{Cons}
  \begin{itemize}
    \item learned coefficients are not very interpretable
    \item not very good at detecting correlated features
  \end{itemize}
\end{frame}

%% Decision Trees
\subsection{Decision Trees}
\begin{frame}
  \frametitle{Decision Trees}
  Decision trees are mostly used for classification problems. They learn a
  \textbf{sequence of conditions} (if/else questions) to find the class of a vector
  $x$.

  Let's suppose we have the following dataset and we want to predict if the game
  can be played (9 yes / 5 no) :
  \begin{table}[htb]
    \centering
    \tiny
      \begin{tabular}{@{}rcccc@{}}
      \cmidrule{2-5}
         & Outlook  & Humidity & Wind   & Play\\
      \midrule
      1  & Sunny    & High     & Weak   & No  \\
      2  & Sunny    & High     & Strong & No  \\
      3  & Overcast & High     & Weak   & Yes \\
      4  & Rain     & High     & Weak   & Yes \\
      5  & Rain     & Normal   & Weak   & Yes \\
      6  & Rain     & Normal   & Strong & No  \\
      7  & Overcast & Normal   & Strong & Yes \\
      8  & Sunny    & High     & Weak   & No  \\
      9  & Sunny    & Normal   & Weak   & Yes \\
      10 & Rain     & Normal   & Weak   & Yes \\
      11 & Sunny    & Normal   & Strong & Yes \\
      12 & Overcast & High     & Strong & Yes \\
      13 & Overcast & Normal   & Weak   & Yes \\
      14 & Rain     & High     & Strong & No  \\
      \bottomrule
    \end{tabular}
    \normalsize
  \end{table}
\end{frame}

\begin{frame}
\frametitle{Decision Trees}
  Many algorithms exist to create a decision tree. One of them is ID3. At each
  step, it computes the entropy of each unused attributes  with :

  \begin{equation*}
    Entropy(S) = \sum_{x \in X} -p(x) \log_{2}p(x)
  \end{equation*}

  where $p(x)$ is the ratio between the number of elements in $x$ and in $S$.
  The attribute that has the lower entropy is chosen as a node in the tree.

  \begin{columns}
    \column{0.48\linewidth}
    \begin{table}[htb]
      \centering
      \tiny
        \begin{tabular}{@{}rcccc@{}}
        \cmidrule{2-5}
           & Outlook  & Humidity & Wind   & Play\\
        \midrule
        1  & Sunny    & High     & Weak   & No  \\
        2  & Sunny    & High     & Strong & No  \\
        3  & Overcast & High     & Weak   & Yes \\
        4  & Rain     & High     & Weak   & Yes \\
        5  & Rain     & Normal   & Weak   & Yes \\
        6  & Rain     & Normal   & Strong & No  \\
        7  & Overcast & Normal   & Strong & Yes \\
        8  & Sunny    & High     & Weak   & No  \\
        9  & Sunny    & Normal   & Weak   & Yes \\
        10 & Rain     & Normal   & Weak   & Yes \\
        11 & Sunny    & Normal   & Strong & Yes \\
        12 & Overcast & High     & Strong & Yes \\
        13 & Overcast & Normal   & Weak   & Yes \\
        14 & Rain     & High     & Strong & No  \\
        \bottomrule
      \end{tabular}
      \normalsize
    \end{table}

    \column{0.48\linewidth}
      \centering
      \includegraphics[height=3.2cm]{img/id3.png}
  \end{columns}
\end{frame}

\begin{frame}
  \frametitle{Decision Trees}
  Decision trees have many parameters to control their size : maximum depth,
  number of leaf, acceptable ratio in a node... They provide a strong
  understandable tool for people not familiar with the ML field.
  \\~\\
  \textbf{Pros}
  \begin{itemize}
    \item can visually be understood
    \item can be used with large datasets
    \item no preprocessing needed (normalization of features)
  \end{itemize}

  \textbf{Cons}
  \begin{itemize}
    \item tend to easily overfit
    \item poor generalization performance
  \end{itemize}
\end{frame}

\end{document}


