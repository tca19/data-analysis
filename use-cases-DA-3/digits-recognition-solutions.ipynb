{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Digits recognition with clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`sklearn` has some built-in datasets. One of them is composed of small images of digits and the corresponding digit it represents (the label)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and visualizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the documentation of the function load_digits.\n",
    "# Use it to get the data (both X and y).\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "#?load_digits\n",
    "# By looking at the documentation, one can see that the\n",
    "# function `load_digits()Ì£` returns a dictionary containing\n",
    "# different data in each of its attributes. The interesting\n",
    "# ones are named `data` (our X) and `target` (our y).\n",
    "# I could write:\n",
    "# X, y = load_digits(return_X_y=True)\n",
    "# which only returns the interesting fields but since I will\n",
    "# need the images for the next question, I load it entirely.\n",
    "data = load_digits()\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples are in this dataset ?\n",
    "print(\"There are {} samples in the digits dataset.\".\n",
    "    format(len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many samples of the classe '7' are in this dataset ?\n",
    "# The condition `y == 7` returns a numpy array of booleans\n",
    "# where values are True each time the label is 7. Then\n",
    "# np.sum() gives the number of occurrences of `True` in the \n",
    "# array, since True is evaluated as 1 and False as 0. The\n",
    "# number of occurrences of True is the number of labels equal\n",
    "# to 7.\n",
    "print(\"There are {} samples of the class 7 in the dataset.\".\n",
    "    format((y == 7).sum()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many features are in each input vector ?\n",
    "print(\"There are {} features in each input vector.\".\n",
    "    format(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print the frequency of each class as an histogram.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# np.histogram() counts the number of values in each bucket,\n",
    "# where the buckets are spread along [0-9] (the possible\n",
    "# clasess). We indicate that we want the same number of\n",
    "# buckets as the number of labels.\n",
    "occurrences = np.histogram(\n",
    "                  y, bins=len(data[\"target_names\"]))[0]\n",
    "plt.bar(data[\"target_names\"], occurrences)\n",
    "\n",
    "# Visualization is better if mininal value on y-axis is not 0\n",
    "plt.ylim(min(occurrences) - 2)\n",
    "\n",
    "# Modify the names on x-axis and y-axis (better readability)\n",
    "plt.xticks(data[\"target_names\"])\n",
    "plt.yticks(sorted(list(set(occurrences))))\n",
    "plt.title(\"Frequency of each class in the digits dataset\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the fonction imshow() of the plt library to represent\n",
    "# as an image:\n",
    "#   - 3 different samples of the class \"9\"\n",
    "#   - 3 different samples of the class \"6\"\n",
    "#   - 3 different samples of the class \"5\"\n",
    "\n",
    "# First, we need to get the images of the \"9\", the \"6\" and\n",
    "# the \"5\". The images are in the `images` field of the `data`\n",
    "# variable, but it contains all types of classes. We use\n",
    "# boolean arrays to only select the images of a certain class.\n",
    "all_9 = data[\"images\"][y == 9]\n",
    "all_6 = data[\"images\"][y == 6]\n",
    "all_5 = data[\"images\"][y == 5]\n",
    "\n",
    "# Print all the images into the same plot (use subplots()\n",
    "# with a size of 3x3 so that the first row contains \"9\", the\n",
    "# second row contains \"6\" and the third row contains \"5\")\n",
    "fig, ax = plt.subplots(3,3, figsize=(10, 10),\n",
    "                       sharex=False, sharey=True)\n",
    "\n",
    "# First row contains \"9\"\n",
    "for i in range(3):\n",
    "    ax[0][i].imshow(all_9[i])\n",
    "    ax[0][i].set_title(\"Image #{} of the digit 9\".format(i))\n",
    "\n",
    "# Second row contains \"6\"\n",
    "for i in range(3):\n",
    "    ax[1][i].imshow(all_6[i])\n",
    "    ax[1][i].set_title(\"Image #{} of the digit 6\".format(i))\n",
    "\n",
    "# Third row contains \"5\"\n",
    "for i in range(3):\n",
    "    ax[2][i].imshow(all_5[i])\n",
    "    ax[2][i].set_title(\"Image #{} of the digit 5\".format(i))\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting digits with K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the dataset into a train and test size with\n",
    "# random_state = 0.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                   random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the class KMeans, create a model that clusterize the \n",
    "# training data. Use the right number of clusters.\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# We indicate that we want the same number of clusters as\n",
    "# the number of classes.\n",
    "model = KMeans(n_clusters=len(data[\"target_names\"]),\n",
    "               random_state=0)\n",
    "model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model learns 2 things:\n",
    "#   - the position of each centroid (model.cluster_centers_)\n",
    "#   - the id of the cluster to which each point belongs (model.labels_)\n",
    "#\n",
    "# Use the above information to build, for each centroid,\n",
    "# the list of points and their label belonging to that \n",
    "# centroid.\n",
    "\n",
    "# We associate each centroid to a list of tuples. Each tuple\n",
    "# is composed of (point_id, label_of_point_id). Each centroid\n",
    "# only has in its list the points that have been associated\n",
    "# to it by the kmeans model.\n",
    "from collections import defaultdict\n",
    "\n",
    "points_near_centroid = defaultdict(list)\n",
    "for point_id in range(len(X_train)):\n",
    "    centroid_assigned = model.labels_[point_id]\n",
    "    points_near_centroid[centroid_assigned].append(\n",
    "        (point_id, y_train[point_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then build a mapping between a cluster id and the class it\n",
    "# represents (use a dictionary for this).\n",
    "\n",
    "# We use what we have built at the previous question. For\n",
    "# each centroid, we iterate over its tuple list and find what\n",
    "# is the most frequent label among the tuples. We can assume\n",
    "# that the most frequent label is the digit class this\n",
    "# centroid represents.\n",
    "from collections import Counter\n",
    "\n",
    "\"\"\"Return the most frequent label in the tuple list. Each\n",
    "tuple is composed of (point_id, label_of_point_id).\"\"\"\n",
    "def most_frequent_label(tuple_list):\n",
    "    all_labels = [t[1] for t in tuple_list]\n",
    "    c = Counter(all_labels)\n",
    "    # most_common() return a list of tuple (elem, occurrence)\n",
    "    # Take the only element, then the first value of tuple\n",
    "    return c.most_common(1)[0][0]\n",
    "\n",
    "label_of_centroid = {}\n",
    "for centroid, tuple_list in points_near_centroid.items():\n",
    "    label_of_centroid[centroid] = most_frequent_label(tuple_list)\n",
    "\n",
    "# We can look at the mapping between a centroid id and the\n",
    "# digit it represents. We can see that it is not perfect,\n",
    "# because there is no centroid representing the digit \"9\"\n",
    "# while two centroids represent the digit \"1\".\n",
    "for k, v in label_of_centroid.items():\n",
    "    print(\"Centroid #{} => Digit #{}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the documentation of the .predict() method of your\n",
    "# model. Use it to find the class predicted by your model\n",
    "# for each point of your test set.\n",
    "\n",
    "centroid_predicted = model.predict(X_test)\n",
    "label_predicted = [label_of_centroid[c]\n",
    "                   for c in centroid_predicted]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the accuracy of your KMeans model on the test set ?\n",
    "\n",
    "# We need to compare the values of the arrays:\n",
    "#   - label_predicted\n",
    "#   - y_test\n",
    "# Each time the values match, the prediction is correct. The\n",
    "# accuracy is the ratio between the number correct\n",
    "# predictions and the number of samples in the test set.\n",
    "correct  = np.sum(np.array(label_predicted) == y_test)\n",
    "accuracy = correct / len(y_test)\n",
    "print(\"Accuracy of the model: {:.2f}.\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining PCA and Kmeans for prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the PCA algorithm to transform the digits data\n",
    "# (all of them i.e. both train and test) into a 3-dimensional\n",
    "# dataset.\n",
    "\n",
    "# First we create a PCA model and feed it with all the X data\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3, random_state=0)\n",
    "pca.fit(X)\n",
    "\n",
    "# Then we use it to transform X_train and X_test. y_train and\n",
    "# y_test do not have to be transformed because they are 1D\n",
    "# arrays (only composed of labels).\n",
    "X_train_3d = pca.transform(X_train)\n",
    "X_test_3d  = pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the reduced dataset on a 3D figure.\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "ax.scatter(X_train_3d[:,0], X_train_3d[:,1], X_train_3d[:,2],\n",
    "           c=y_train, s=10, label=\"Train data\")\n",
    "ax.scatter(X_test_3d[:,0], X_test_3d[:,1], X_test_3d[:,2],\n",
    "           c=y_test, s=40, marker=\"x\", label=\"Test data\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train another KMeans on the 3D dataset. Compare its\n",
    "# accuracy against the previous KMeans model. Do you get\n",
    "# better results?\n",
    "\n",
    "# Train a kmeans model on the 3d reduced dataset\n",
    "model_3d = KMeans(n_clusters=len(data[\"target_names\"]),\n",
    "                  random_state=0)\n",
    "model_3d.fit(X_train_3d)\n",
    "\n",
    "# Build the mapping centroid_id <-> digit it represents\n",
    "labels_near_centroid_3d = defaultdict(list)\n",
    "for point_id in range(len(X_train_3d)):\n",
    "    c = model_3d.labels_[point_id]\n",
    "    labels_near_centroid_3d[c].append(y_train[point_id])\n",
    "    \n",
    "label_of_centroid_3d = {}\n",
    "for centroid, labels in labels_near_centroid_3d.items():\n",
    "    most_freq_label = Counter(labels).most_common(1)[0][0]\n",
    "    label_of_centroid_3d[centroid] = most_freq_label\n",
    "\n",
    "# We can look at the mapping between a centroid id and the \n",
    "# digit it represents. This time, the class \"9\" has a mapping\n",
    "# but the class \"5\" does not. This time, the digit \"7\" has\n",
    "# two different centroid representing it.\n",
    "for k, v in label_of_centroid_3d.items():\n",
    "    print(\"Centroid #{} => Digit #{}\".format(k, v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the accuracy of the new model\n",
    "label_predicted_3d = np.array([label_of_centroid_3d[c]\n",
    "                             for c in model_3d.predict(X_test_3d)])\n",
    "accuracy_3d = np.sum(label_predicted_3d == y_test) / len(y_test)\n",
    "print(\"Accuracy of the model on 3d data: {:.2f}\".format(\n",
    "    accuracy_3d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the accuracy with the 3d dataset is worse than with the non reduced dataset: 0.67 vs. 0.78. Several possible explanations:\n",
    "* the dataset is hard to classify: we printed the images of some 9, 6, 5 and some are hard to differenciate if we don't have the correponding labels (some \"5\" can be seen as a \"9\"). The model therefore needs as much as possible information to perform well, and cannot do a good classification with only 3 dimensions per input vector.\n",
    "* the dataset already has a small number of features: the images only have 64 pixels. PCA is usually used when the number of features is larger than 100. Reducing it to only 3 dimensions induce a large loss of information.\n",
    "* the number of clusters is not correct: we set the number of cluster to match the number of classes (10). But some samples with different digits are close to each other (like the \"5\" and the \"9\") and then belong to the same cluster, even if they do not have the same class. With a greater number of clusters, the size of each cluster is reduced and the probability of two different digits belonging to the same cluster is reduced, thus the accuracy should increase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
